{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "class A2C(nn.Module):\n",
    "\n",
    "    def __init__(self, env, hidden_size=128, gamma=.99, random_seed=None):\n",
    "        \"\"\"\n",
    "        Assumes fixed continuous observation space\n",
    "        and fixed discrete action space (for now)\n",
    "\n",
    "        :param env: target gym environment\n",
    "        :param gamma: the discount factor parameter for expected reward function :float\n",
    "        :param random_seed: random seed for experiment reproducibility :float, int, str\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if random_seed:\n",
    "            env.seed(random_seed)\n",
    "            torch.manual_seed(random_seed)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_size = len(env.observation_space.sample().flatten())\n",
    "        self.out_size = self.env.action_space.n\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self.in_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, self.out_size)\n",
    "        ).double()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.in_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        ).double()\n",
    "\n",
    "    def train_env_episode(self, render=False):\n",
    "        \"\"\"\n",
    "        Runs one episode and collects critic values, expected return,\n",
    "        :return: A tensor with total/expected reward, critic eval, and action information\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        critic_vals = []\n",
    "        action_lp_vals = []\n",
    "\n",
    "        # Run episode and save information\n",
    "\n",
    "        observation = self.env.reset()\n",
    "        observation = observation[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            observation = torch.from_numpy(observation).double()\n",
    "\n",
    "            # Get action from actor\n",
    "            action_logits = self.actor(observation)\n",
    "\n",
    "            action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "            # Get action probability\n",
    "            action_log_prob = action_logits[action]\n",
    "\n",
    "            # Get value from critic\n",
    "            pred = torch.squeeze(self.critic(observation).view(-1))\n",
    "\n",
    "            # Write prediction and action/probabilities to arrays\n",
    "            action_lp_vals.append(action_log_prob)\n",
    "            critic_vals.append(pred)\n",
    "\n",
    "            # Send action to environment and get rewards, next state\n",
    "\n",
    "            observation, reward, done, _, info = self.env.step(action.item())\n",
    "            rewards.append(torch.tensor(reward).double())\n",
    "\n",
    "        total_reward = sum(rewards)\n",
    "\n",
    "        # Convert reward array to expected return and standardize\n",
    "        for t_i in range(len(rewards)):\n",
    "\n",
    "            for t in range(t_i + 1, len(rewards)):\n",
    "                rewards[t_i] += rewards[t] * (self.gamma ** (t_i - t))\n",
    "\n",
    "        # Convert output arrays to tensors using torch.stack\n",
    "        def f(inp):\n",
    "            return torch.stack(tuple(inp), 0)\n",
    "\n",
    "        # Standardize rewards\n",
    "        rewards = f(rewards)\n",
    "        rewards = (rewards - torch.mean(rewards)) / (torch.std(rewards) + .000000000001)    # Normalize A2C\n",
    "\n",
    "        return rewards, f(critic_vals), f(action_lp_vals), total_reward\n",
    "\n",
    "    def test_env_episode(self, render=True):\n",
    "        \"\"\"\n",
    "        Run an episode of the environment in test mode\n",
    "        :param render: Toggle rendering of environment :bool\n",
    "        :return: Total reward :int\n",
    "        \"\"\"\n",
    "        observation = self.env.reset()\n",
    "        observation = observation[0]\n",
    "        rewards = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                plt.imshow(self.env.render(mode='rgb_array'))\n",
    "                display.display(plt.gcf())    \n",
    "                display.clear_output(wait=True)\n",
    "\n",
    "            observation = torch.from_numpy(observation).double()\n",
    "\n",
    "            # Get action from actor\n",
    "            action_logits = self.actor(observation)\n",
    "            action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "            observation, reward, done, info = self.env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "\n",
    "        return sum(rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss(action_p_vals, G, V, critic_loss=nn.SmoothL1Loss()):\n",
    "        \"\"\"\n",
    "        Actor Advantage Loss, where advantage = G - V\n",
    "        Critic Loss, using mean squared error\n",
    "        :param critic_loss: loss function for critic   :Pytorch loss module\n",
    "        :param action_p_vals: Action Log Probabilities  :Tensor\n",
    "        :param G: Actual Expected Returns   :Tensor\n",
    "        :param V: Predicted Expected Returns    :Tensor\n",
    "        :return: Actor loss tensor, Critic loss tensor  :Tensor\n",
    "        \"\"\"\n",
    "        assert len(action_p_vals) == len(G) == len(V)\n",
    "        advantage = G - V.detach()\n",
    "        return -(torch.sum(action_p_vals * advantage)), critic_loss(G, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cexu/.local/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/cexu/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward during episodes 0-100 is 46.26\n",
      "Average reward during episodes 100-200 is 101.46\n",
      "Average reward during episodes 200-300 is 98.18\n",
      "Average reward during episodes 300-400 is 196.51\n",
      "Solved CartPole-v0 with average reward 196.51\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train\n",
    "Cart-pole reinforcement learning environment:\n",
    "Agent learns to balance a pole on a cart\n",
    "\n",
    "a2c: Agent uses Advantage Actor Critic algorithm\n",
    "\n",
    "\"\"\"\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "LR = .01  # Learning rate\n",
    "SEED = None  # Random seed for reproducibility\n",
    "MAX_EPISODES = 10000  # Max number of episodes\n",
    "\n",
    "# Init actor-critic agent\n",
    "agent = A2C(gym.make('CartPole-v1'), random_seed=SEED)\n",
    "\n",
    "# Init optimizers\n",
    "actor_optim = optim.Adam(agent.actor.parameters(), lr=LR)\n",
    "critic_optim = optim.Adam(agent.critic.parameters(), lr=LR)\n",
    "\n",
    "#\n",
    "# Train\n",
    "#\n",
    "\n",
    "r = []  # Array containing total rewards\n",
    "avg_r = 0  # Value storing average reward over last 100 episodes\n",
    "\n",
    "for i in range(MAX_EPISODES):\n",
    "    critic_optim.zero_grad()\n",
    "    actor_optim.zero_grad()\n",
    "\n",
    "    rewards, critic_vals, action_lp_vals, total_reward = agent.train_env_episode(render=False)\n",
    "    r.append(total_reward)\n",
    "\n",
    "    l_actor, l_critic = agent.compute_loss(action_p_vals=action_lp_vals, G=rewards, V=critic_vals)\n",
    "\n",
    "    l_actor.backward()\n",
    "    l_critic.backward()\n",
    "\n",
    "    actor_optim.step()\n",
    "    critic_optim.step()\n",
    "\n",
    "    # Check average reward every 100 episodes, print, and end script if solved\n",
    "    if len(r) >= 100:  # check average every 100 episodes\n",
    "\n",
    "        episode_count = i - (i % 100)\n",
    "        prev_episodes = r[len(r) - 100:]\n",
    "        avg_r = sum(prev_episodes) / len(prev_episodes)\n",
    "        if len(r) % 100 == 0:\n",
    "            print(f'Average reward during episodes {episode_count}-{episode_count+100} is {avg_r.item()}')\n",
    "        if avg_r > 195:\n",
    "            print(f\"Solved CartPole-v0 with average reward {avg_r.item()}\")\n",
    "            break\n",
    "\n",
    "torch.save(agent.actor.state_dict(), 'actor_state_dict')\n",
    "torch.save(agent.critic.state_dict(), 'critic_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "agent = A2C(gym.make('CartPole-v1'), random_seed=SEED)\n",
    "\n",
    "# Init optimizers\n",
    "actor_optim = optim.Adam(agent.actor.parameters(), lr=LR)\n",
    "critic_optim = optim.Adam(agent.critic.parameters(), lr=LR)\n",
    "agent.actor.load_state_dict(torch.load('actor_state_dict'))\n",
    "agent.critic.load_state_dict(torch.load('critic_state_dict'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "render() got an unexpected keyword argument 'render_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m agent\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m    plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m    display\u001b[38;5;241m.\u001b[39mdisplay(plt\u001b[38;5;241m.\u001b[39mgcf())    \n\u001b[1;32m     10\u001b[0m    display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: render() got an unexpected keyword argument 'render_mode'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# Test\n",
    "#\n",
    "    \n",
    "agent.env.reset()\n",
    "\n",
    "for i in range(25):\n",
    "   plt.imshow(agent.env.render(render_mode='rgb_array'))\n",
    "   display.display(plt.gcf())    \n",
    "   display.clear_output(wait=True)\n",
    "   agent.env.step(agent.env.action_space.sample()) # take a random action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "render() got an unexpected keyword argument 'rendre_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_modes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m agent\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mrendre_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: render() got an unexpected keyword argument 'rendre_mode'"
     ]
    }
   ],
   "source": [
    "agent.env.metadata['render_modes']\n",
    "agent.env.reset()\n",
    "agent.env.render( rendre_mode='rgb_array' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
